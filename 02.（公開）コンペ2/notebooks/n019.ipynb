{"cells":[{"cell_type":"markdown","metadata":{},"source":["### 令和6年7月6日(土)"]},{"cell_type":"markdown","metadata":{},"source":["#### 不要な特徴量の削除\n"]},{"cell_type":"markdown","metadata":{},"source":["#### 1. 読み込み"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QShif6ZLCnmC"},"outputs":[],"source":["# ライブラリの読み込み\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_auc_score\n","from sklearn.neural_network import MLPClassifier\n","from lightgbm import LGBMClassifier\n","from catboost import CatBoostClassifier\n","import lightgbm as lgbm\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","from hyperopt import hp, fmin, tpe, Trials\n","import pickle\n","import os\n","import sys\n","sys.path.append('../')\n","from my_utils import MyUtils\n","from params import Params\n","from model.multi_model import Lgbm_hyperopt, Xgb_hyperopt, Cat_hyperopt\n","import warnings\n","warnings.filterwarnings('ignore')\n","INPUT_DIR = \"../input/\"\n","RANDOM_STATE = 10\n","OUTPUT_PATH = \"C:/Users/gwsgs/workSpace/GCIcomp2/02.（公開）コンペ2/output/\"\n","remove_outliers_columns = [ \"AMT_INCOME_TOTAL\", \"AMT_ANNUITY\", \"DAYS_ID_PUBLISH\",]\n","exel_path = 'score_record.xlsx' \n","utils = MyUtils(input_path=INPUT_DIR, output_path=OUTPUT_PATH, exel_path=exel_path) \n","my_utils = MyUtils(input_path=INPUT_DIR, output_path=OUTPUT_PATH, exel_path=exel_path)\n","my_params = Params() "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXM13YCeCnmE"},"outputs":[],"source":["# データの読み込み\n","# INPUT_DIRにtrain.csvなどのデータを置いているディレクトリを指定してください。\n","INPUT_DIR = \"../input/\"\n","import datetime\n","time = datetime.datetime.now().strftime('%Y%m%d%H%M')\n","train = pd.read_csv(INPUT_DIR + \"train.csv\")\n","test = pd.read_csv(INPUT_DIR + \"test.csv\")\n","sample_sub = pd.read_csv(INPUT_DIR + \"sample_submission.csv\")\n","X_train = train.drop('TARGET',axis=1)\n","y_train = train['TARGET']"]},{"cell_type":"markdown","metadata":{"id":"rsPYkguwCnmO"},"source":["#### 2. 前処理と特徴量作成\n","ここでは、上記の可視化と分析でわかったことを踏まえて、前処理と特徴量の作成を行います。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1636420635714,"user":{"displayName":"Takuya Fukushima","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhR4LDFC_RMoC8DpM73KIZhKgCXA-oZ-HDU8R2wtw=s64","userId":"15264897430699553304"},"user_tz":-540},"id":"yO-iJnwLCnmO","outputId":"d5a6dca3-a698-4423-f7cc-69888b4d703b"},"outputs":[],"source":["# 欠損値の補完を行う\n","# FLAF_OWN_CARの欠損値をラベルエンコーディングで補完\n","train[\"FLAG_OWN_CAR\"].fillna('Unknown', inplace=True)\n","train[\"FLAG_OWN_CAR\"] = train[\"FLAG_OWN_CAR\"].map(train['FLAG_OWN_CAR'].value_counts().rank(ascending=False, method='first'))\n","test[\"FLAG_OWN_CAR\"].fillna('Unknown', inplace=True)\n","test[\"FLAG_OWN_CAR\"] = test[\"FLAG_OWN_CAR\"].map(test['FLAG_OWN_CAR'].value_counts().rank(ascending=False, method='first'))\n","# FLAG_OWN_REALTYの欠損値はをラベルカウントエンコーディングで補完\n","train[\"FLAG_OWN_REALTY\"].fillna('Unknown', inplace=True)\n","train[\"FLAG_OWN_REALTY\"] = train[\"FLAG_OWN_REALTY\"].map(train[\"FLAG_OWN_REALTY\"].value_counts().rank(ascending=False, method='first'))\n","test[\"FLAG_OWN_REALTY\"].fillna('Unknown', inplace=True)\n","test[\"FLAG_OWN_REALTY\"] = test[\"FLAG_OWN_REALTY\"].map(test[\"FLAG_OWN_REALTY\"].value_counts().rank(ascending=False, method='first'))\n","# AMT_ANNUITYの欠損値を中央値で補完\n","train[\"AMT_ANNUITY\"].fillna(train[\"AMT_ANNUITY\"].median(), inplace=True)\n","test[\"AMT_ANNUITY\"].fillna(train[\"AMT_ANNUITY\"].median(), inplace=True)\n","# AMT_GOODS_PRICEの欠損値を中央値で補完\n","train[\"AMT_GOODS_PRICE\"].fillna(train[\"AMT_GOODS_PRICE\"].median(), inplace=True)\n","test[\"AMT_GOODS_PRICE\"].fillna(train[\"AMT_GOODS_PRICE\"].median(), inplace=True)\n","# NAME_TYPE_SUITEの欠損値をラベルカウントエンコーディングで補完\n","train[\"NAME_TYPE_SUITE\"].fillna(\"Unknown\", inplace=True)\n","train[\"NAME_TYPE_SUITE\"] = train[\"NAME_TYPE_SUITE\"].map(train[\"NAME_TYPE_SUITE\"].value_counts().rank(ascending=False, method='first'))\n","test[\"NAME_TYPE_SUITE\"].fillna(\"Unknown\", inplace=True)\n","test[\"NAME_TYPE_SUITE\"] = test[\"NAME_TYPE_SUITE\"].map(test[\"NAME_TYPE_SUITE\"].value_counts().rank(ascending=False, method='first'))\n","# OCCUPATION_TYPEの欠損値をラベルエンコーディングで補完\n","train[\"OCCUPATION_TYPE\"].fillna(\"Unknown\", inplace=True)\n","train[\"OCCUPATION_TYPE\"] = train[\"OCCUPATION_TYPE\"].map(train[\"OCCUPATION_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","test[\"OCCUPATION_TYPE\"].fillna(\"Unknown\", inplace=True)\n","test[\"OCCUPATION_TYPE\"] = test[\"OCCUPATION_TYPE\"].map(test[\"OCCUPATION_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","# CNT_FAM_MEMBERSの欠損値を中央値で補完\n","train[\"CNT_FAM_MEMBERS\"].fillna(train[\"CNT_FAM_MEMBERS\"].median(), inplace=True)\n","test[\"CNT_FAM_MEMBERS\"].fillna(train[\"CNT_FAM_MEMBERS\"].median(), inplace=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# カテゴリカルデータを数値に変換\n","# NAME_CONTRACT_TYPEのラベルエンコーディング\n","train[\"NAME_CONTRACT_TYPE\"] = train[\"NAME_CONTRACT_TYPE\"].map({\"Cash loans\": 0, \"Revolving loans\": 1})\n","test[\"NAME_CONTRACT_TYPE\"] = test[\"NAME_CONTRACT_TYPE\"].map({\"Cash loans\": 0, \"Revolving loans\": 1})\n","# CODE_GENDERのラベルエンコーディング(男性=0, 女性=1, XNA=0)\n","train[\"CODE_GENDER\"] = train[\"CODE_GENDER\"].map({\"M\": 0, \"F\": 1, \"XNA\": 0})\n","test[\"CODE_GENDER\"] = test[\"CODE_GENDER\"].map({\"M\": 0, \"F\": 1, \"XNA\": 0})\n","# NAME_INCOME_TYPEのラベルカウントエンコーディング\n","train[\"NAME_INCOME_TYPE\"] = train[\"NAME_INCOME_TYPE\"].map(train[\"NAME_INCOME_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","test[\"NAME_INCOME_TYPE\"] = test[\"NAME_INCOME_TYPE\"].map(test[\"NAME_INCOME_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","# NAME_EDUCATION_TYPEのラベルカウントエンコーディング\n","train[\"NAME_EDUCATION_TYPE\"] = train[\"NAME_EDUCATION_TYPE\"].map(train[\"NAME_EDUCATION_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","test[\"NAME_EDUCATION_TYPE\"] = test[\"NAME_EDUCATION_TYPE\"].map(test[\"NAME_EDUCATION_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","# NAME_FAMILY_STATUSのラベルカウントエンコーディング\n","train[\"NAME_FAMILY_STATUS\"] = train[\"NAME_FAMILY_STATUS\"].map(train[\"NAME_FAMILY_STATUS\"].value_counts().rank(ascending=False, method='first'))\n","test[\"NAME_FAMILY_STATUS\"] = test[\"NAME_FAMILY_STATUS\"].map(test[\"NAME_FAMILY_STATUS\"].value_counts().rank(ascending=False, method='first'))\n","# NAME_HOUSING_TYPEのラベルカウントエンコーディング\n","train[\"NAME_HOUSING_TYPE\"] = train[\"NAME_HOUSING_TYPE\"].map(train[\"NAME_HOUSING_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","test[\"NAME_HOUSING_TYPE\"] = test[\"NAME_HOUSING_TYPE\"].map(test[\"NAME_HOUSING_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","# ORGANIZATIONのラベルカウントエンコーディング\n","train[\"ORGANIZATION_TYPE\"] = train[\"ORGANIZATION_TYPE\"].map(train[\"ORGANIZATION_TYPE\"].value_counts().rank(ascending=False, method='first'))\n","test[\"ORGANIZATION_TYPE\"] = test[\"ORGANIZATION_TYPE\"].map(test[\"ORGANIZATION_TYPE\"].value_counts().rank(ascending=False, method='first'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 特徴量の作成\n","# # DAYS_LAST_PHONE_CHANGEを年に変換\n","# train[\"YEARS_PHONR_CHANGE\"] = train[\"DAYS_LAST_PHONE_CHANGE\"] / -365\n","# test[\"YEARS_PHONR_CHANGE\"] = test[\"DAYS_LAST_PHONE_CHANGE\"] / -365\n","# # DAYS_EMPLOYEDを年に変換\n","# train[\"YEARS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 365\n","# test[\"YEARS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 365\n","# # DAYS_REGISTRATIONを年に変換\n","# train[\"YEARS_REGISTRATION\"] = train[\"DAYS_REGISTRATION\"] / -365\n","# test[\"YEARS_REGISTRATION\"] = test[\"DAYS_REGISTRATION\"] / -365\n","# # DAYS_ID_PUBLISHを年に変換\n","# train[\"YEARS_ID_PUBLISH\"] = train[\"DAYS_ID_PUBLISH\"] / -365\n","# test[\"YEARS_ID_PUBLISH\"] = test[\"DAYS_ID_PUBLISH\"] / -365\n","# OCCUPATION_TYPEのLow-skill Laborersを1、それ以外を0に変換。新しく特徴量[is_low_skill]を作成\n","train[\"is_low_skill\"] = train[\"OCCUPATION_TYPE\"].apply(lambda x: 1 if x == \"Low-skill Laborers\" else 0)\n","test[\"is_low_skill\"] = test[\"OCCUPATION_TYPE\"].apply(lambda x: 1 if x == \"Low-skill Laborers\" else 0)\n","# 参考 https://www.kaggle.com/competitions/home-credit-default-risk/discussion/64821\n","# AMT_CREDITとAMT_ANNUITYの比率\n","train[\"CREDIT_TO_ANNUITY_RATIO\"] = train[\"AMT_CREDIT\"] / train[\"AMT_ANNUITY\"]\n","test[\"CREDIT_TO_ANNUITY_RATIO\"] = test[\"AMT_CREDIT\"] / test[\"AMT_ANNUITY\"]\n","# AMT_CREDITとAMT_GOODS_PRICEの比率\n","train[\"CREDIT_TO_GOODS_RATIO\"] = train[\"AMT_CREDIT\"] / train[\"AMT_GOODS_PRICE\"]\n","test[\"CREDIT_TO_GOODS_RATIO\"] = test[\"AMT_CREDIT\"] / test[\"AMT_GOODS_PRICE\"]\n","# AMT_CREDITとAMT_ANNUITYの比率\n","train[\"AMT_CREDIT_TO_AMT_ANNUITY\"] = train[\"AMT_CREDIT\"] / train[\"AMT_ANNUITY\"]\n","test[\"AMT_CREDIT_TO_AMT_ANNUITY\"] = test[\"AMT_CREDIT\"] / test[\"AMT_ANNUITY\"]\n","# DAYS_BIRTH/ -365\n","train[\"YEARS_BIRTH\"] = train[\"DAYS_BIRTH\"] / -365\n","test[\"YEARS_BIRTH\"] = test[\"DAYS_BIRTH\"] / -365"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 不要な特徴量の削除\n","train.drop(['REGION_RATING_CLIENT', \n","            'REG_REGION_NOT_WORK_REGION',\n","            'REG_CITY_NOT_WORK_CITY',\n","            'LIVE_REGION_NOT_WORK_REGION'], axis=1, inplace=True)\n","test.drop(['REGION_RATING_CLIENT',\n","            'REG_REGION_NOT_WORK_REGION',\n","            'REG_CITY_NOT_WORK_CITY',\n","            'LIVE_REGION_NOT_WORK_REGION'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joie-z89KBdg"},"outputs":[],"source":["# 目的変数と説明変数に分割\n","X = train.drop(\"TARGET\", axis=1).values\n","y = train[\"TARGET\"].values\n","X_test = test.values\n","# 訓練データと評価データに分割\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n","# 訓練データを3分割\n","x_list = list(np.array_split(X_train, 3))\n","y_list = list(np.array_split(y_train, 3))"]},{"cell_type":"markdown","metadata":{},"source":["## 3. 予測"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lgb_n_iter = 1000\n","xgb_n_iter = 10\n","cat_n_iter = 1000\n","lr_n_iter =  1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["time = datetime.datetime.now().strftime('%Y%m%d%H%M')\n","# pickleファイル用のディレクトリを作成\n","if not os.path.exists(OUTPUT_PATH + time + \"_pickle\"):\n","    os.makedirs(OUTPUT_PATH + time + \"_pickle\", exist_ok=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["file_name_list= ['lgb_2_best' , 'lgb_6_best', 'lgb_10']\n","# pickleファイルの読み込み\n","model_list = []\n","for file_name in file_name_list:\n","    with open(OUTPUT_PATH + time + '_pickle/'+file_name + '.pickle', mode='rb') as f:\n","        model_list.append(pickle.load(f))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if 'lgb_2_best' not in model_list:\n","    lgbm_2_hyper = Lgbm_hyperopt(X_train, y_train, X_valid, y_valid, \n","                        params=my_params.lgb, n_iter=lgb_n_iter, random_state=RANDOM_STATE, max_depth=2)\n","    lgb_2_best = lgbm_2_hyper.hyper_param()\n","    del lgbm_2_hyper\n","    print(lgb_2_best)\n","    my_utils.save_pickle(lgb_2_best, f\"{time}_pickle/lgb_2_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lgb_2_best = {\n","    'bagging_freq': 4,\n","    'bagging_fraction': 0.939350506933662,\n","    'feature_fraction': 0.507231264497246,\n","    'learning_rate': 0.175650897441088,\n","    'min_child_samples': 20,\n","    'num_leaves': 6,\n","    'reg_lamb': 30.6036201910942,\n","    'n_estimators': 1152,\n","    'min_data_in_leaf': 29,\n","}\n","my_utils.save_pickle(lgb_2_best, f\"{time}_pickle/lgb_2_best\")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  5%|▍         | 97/2000 [09:12<3:26:18,  6.50s/trial, best loss: -0.7635495413990055]"]}],"source":["if 'lgb_6_best' not in model_list:\n","    lgbm_6_hyper = Lgbm_hyperopt(X_train, y_train, X_valid, y_valid, \n","                        params=my_params.lgb, n_iter=lgb_n_iter, random_state=RANDOM_STATE, max_depth=6)\n","    lgb_6_best = lgbm_6_hyper.hyper_param()\n","    del lgbm_6_hyper\n","    print(lgb_6_best)\n","    my_utils.save_pickle(lgb_6_best, f\"{OUTPUT_PATH}{time}_pickle/lgb_6_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if 'lgb_10_best' not in model_list:\n","    lgbm_10_hyper = Lgbm_hyperopt(X_train, y_train, X_valid, y_valid, \n","                        params=my_params.lgb, n_iter=lgb_n_iter, random_state=RANDOM_STATE, max_depth=10)\n","    lgb_10_best = lgbm_10_hyper.hyper_param()\n","    del lgbm_10_hyper\n","    print(lgb_10_best)\n","    my_utils.save_pickle(lgb_10_best, f\"{OUTPUT_PATH}{time}_pickle/lgb_10_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cat_hyper = Cat_hyperopt(X_train, y_train, X_valid, y_valid,\n","                        params=my_params.cat, n_iter=cat_n_iter, random_state=RANDOM_STATE)\n","cat_best = cat_hyper.hyper_param()\n","del cat_hyper\n","print(cat_best)\n","my_utils.save_pickle(cat_best, f\"{OUTPUT_PATH}{time}_pickle/cat_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgb_hyper = Xgb_hyperopt(X_train, y_train, X_valid, y_valid,\n","                    params=my_params.xgb, n_iter=xgb_n_iter, random_state=RANDOM_STATE)\n","xgb_best = xgb_hyper.hyper_param()\n","del xgb_hyper\n","print(xgb_best)\n","my_utils.save_pickle(xgb_best, f\"{OUTPUT_PATH}{time}_pickle/xgb_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(xgb_best)\n","my_utils.save_pickle(xgb_best, f\"xgb_best_{time}.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_dict = []\n","best_dict.append(lgb_2_best)\n","best_dict.append(lgb_6_best)\n","best_dict.append(lgb_10_best)\n","best_dict.append(xgb_best)\n","print(\"lgb_2_best: \", lgb_2_best)\n","print(\"lgb_6_best: \", lgb_6_best)\n","print(\"lgb_10_best: \", lgb_10_best)\n","print(\"xgb_best: \", xgb_best)\n","print(y_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_list[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# lgb_2_best = model_list[0]\n","# lgb_6_best = model_list[1]\n","# lgb_10_best = model_list[2]\n","# スタッキングhttps://rin-effort.com/2020/02/09/learn-stacking/\n","# LightGBM max_depth2\n","lgb_2 = LGBMClassifier(\n","        learning_rate = lgb_2_best['learning_rate'],\n","        objective = my_params.lgb['objective'],\n","        max_depth = 2,\n","        min_child_samples = lgb_2_best['min_child_samples'],\n","        n_estimators = lgb_2_best['n_estimators'],\n","        num_leaves = lgb_2_best['num_leaves'],\n","        reg_lambda= lgb_2_best['reg_lamb'],\n","        # sample_sub = lgb_2_best['subsample'],\n","        # sample_bytree = lgb_2_best['colsample_bytree'],\n","        bagging_freq = lgb_2_best['bagging_freq'],\n","        bagging_fraction = lgb_2_best['bagging_fraction'],\n","        feature_fraction = lgb_2_best['feature_fraction'],\n","        min_data_in_leaf = lgb_2_best['min_data_in_leaf'],\n","        verbose = -1,\n","        random_state = RANDOM_STATE,)\n","\n","# # LightGBM max_depth6\n","# lgb_6 = LGBMClassifier(\n","#         learning_rate = lgb_6_best['learning_rate'],\n","#         objective = my_params.lgb['objective'],\n","#         max_depth = 6,\n","#         # min_child_samples = best['min_child_samples'],\n","#         n_estimators = lgb_6_best['n_estimators'],\n","#         num_leaves = lgb_6_best['num_leaves'],\n","#         reg_lambda= lgb_6_best['reg_lamb'],\n","#         # sample_sub = lgb_6_best['subsample'],\n","#         # sample_bytree = lgb_6_best['colsample_bytree'],\n","#         bagging_freq = lgb_6_best['bagging_freq'],\n","#         bagging_fraction = lgb_6_best['bagging_fraction'],\n","#         feature_fraction = lgb_6_best['feature_fraction'],\n","#         min_data_in_leaf = lgb_6_best['min_data_in_leaf'],\n","#         verbose = -1,\n","#         random_state = RANDOM_STATE,)\n","# # LightGBM max_depth10\n","# lgb_10 = LGBMClassifier(\n","#         learning_rate = lgb_10_best['learning_rate'],\n","#         objective = my_params.lgb['objective'],\n","#         max_depth = 10,\n","#         # min_child_samples = best['min_child_samples'],\n","#         n_estimators = lgb_10_best['n_estimators'],\n","#         num_leaves = lgb_10_best['num_leaves'],\n","#         reg_lambda= lgb_10_best['reg_lamb'],\n","#         bagging_freq = lgb_10_best['bagging_freq'],\n","#         # sample_sub = lgb_10_best['subsample'],\n","#         # sample_bytree = lgb_10_best['colsample_bytree'],\n","#         bagging_fraction = lgb_10_best['bagging_fraction'],\n","#         feature_fraction = lgb_10_best['feature_fraction'],\n","#         min_data_in_leaf = lgb_10_best['min_data_in_leaf'],\n","#         verbose = -1,\n","#         random_state = RANDOM_STATE,)\n","# # CatBoost\n","# cat = CatBoostClassifier(\n","#         learning_rate = cat_best['learning_rate'],\n","#         num_boost_round = cat_best['num_boost_round'],\n","#         depth= cat_best['depth'],\n","#         verbose = 0,\n","#         random_state = RANDOM_STATE,)\n","# # # XGBoost\n","# # xgb = XGBClassifier(\n","# #         learning_rate = xgb_best['learning_rate'],\n","# #         n_estimators = xgb_best['n_estimators'],\n","# #         max_depth = xgb_best['max_depth'],\n","# #         subsample = xgb_best['subsample'],\n","# #         colsample_bytree = xgb_best['colsample_bytree'],\n","# #         gamma = xgb_best['gamma'],\n","# #         min_child_weight = xgb_best['min_child_weight'],\n","# #         verbose = 0,\n","# #         random_state = RANDOM_STATE,)\n","\n","# # predicts = [xgb_best, lgb_2_best, lgb_6_best, lgb_10_best]\n","# predicts = [lgb_2_best, lgb_6_best, lgb_10_best, cat_best]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 訓練データと評価データに分割\n","# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n","# 訓練データを3分割\n","x_list = list(np.array_split(X_train, 3))\n","y_list = list(np.array_split(y_train, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 予測値をターゲットとともにnp.arrayでconcat\n","pred_list = []\n","result_list = []\n","valid_list = []\n","# model_list = [lgb_2, lgb_6,lgb_10, cat]\n","model_list = [lgb_2, ]\n","for i, model in enumerate(model_list):\n","        print(f\"model_start: {i}\")\n","        for k, (x, y) in enumerate(zip(x_list, y_list)):\n","                model.fit(x, y)\n","                # validデータを用いて評価\n","                train_pred = model.predict_proba(x)[:, 1]\n","                valid_pred = model.predict_proba(X_valid)[:, 1]\n","                test_pred = model.predict_proba(X_test)[:, 1]\n","                print(f\"{i}_{k}model:train roc_auc: {roc_auc_score(y, train_pred)}\")\n","                print(f\"valid roc_auc: {roc_auc_score(y_valid, valid_pred)}\")\n","                vaild_stack = np.stack([valid_pred, y_valid], axis=1)\n","                # validデータに対する予測値を結合\n","                if k == 0:\n","                        vaild_pred_np = vaild_stack\n","                else:\n","                        vaild_pred_np = np.concatenate([vaild_pred_np, vaild_stack], axis=0)\n","                # model毎の予測値をリストに追加\n","                pred_list.append(test_pred)\n","        # モデル毎のvalidデータに対する予測値をリストに追加\n","        valid_list.append(vaild_pred_np)\n","        # モデル毎の予測値の平均をとる\n","        test_mean = np.mean(pred_list, axis=0)\n","        # モデル毎の予測値の平均をリストに追加\n","        result_list.append(test_mean)\n","print(\"finish\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# pred_npを用いて、スタッキング用のデータセットを作成\n","\n","x_valid = vaild_pred_np[:,0].reshape(-1,1)\n","y_valid = vaild_pred_np[:, 1].reshape(-1,1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ロジスティック回帰を用いてスタッキング\n","from sklearn.linear_model import LogisticRegression\n","lr = LogisticRegression()\n","lr.fit(x_valid, y_valid)\n","# 予測\n","test_preds = np.column_stack(result_list).mean(axis=1).reshape(-1,1)\n","meta_pred = lr.predict_proba(test_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["meta_pred[:].max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dic = {}\n","for i,  column in enumerate(test.columns):\n","    dic[i] = column\n","dic"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# lgbm.plot_importance(lgb,figsize=(10, 10))"]},{"cell_type":"markdown","metadata":{"id":"tn_kdvWYCnmQ"},"source":["#### 4. 予測結果の作成\n","最後にテストデータに対して予測を行い、提出用のcsvファイルを作成します。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IPNn-_ZCnmQ"},"outputs":[],"source":["# テストデータに対する予測値の作成\n","# pred = lgb.predict_proba(X_test, num_iteration=lgb.best_iteration)[:, 1]\n","lgb_pred = lgb_2.predict_proba(X_test)[:, 1]\n","xgb_pred = xgb.predict_proba(X_test)[:, 1]\n","# lr_pred = lr.predict_proba(X_test_std)[:, 1]                    \n","# アンサンブル\n","pred = lgb_pred * 0.5 + xgb_pred * 0.5\n","# pred = lgb_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# 予測結果を提出用のフォーマットに格納\n","sample_sub['TARGET'] = meta_pred[:,0].reshape(-1)\n","# meta_pred[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cvvLh7-eW7ll"},"outputs":[],"source":["# 提出用のcsvファイルを作成\n","# formatを指定して現在時刻を取得（yyyyMMddhhmm）\n","\n","sample_sub.to_csv(f'{OUTPUT_PATH}submit/{time}.csv',index=False)\n","print(f\"output: {OUTPUT_PATH}submit/{time}.csv\")"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"3fc12855e5119aa7119eb8b28b2c79e4453dd0444ad04c81a8c18197ce5b843e"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('venv': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
